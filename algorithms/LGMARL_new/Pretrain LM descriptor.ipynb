{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16a95292",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c30253ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obs</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.7857142857142857, 0.21428571428571427, 0.0,...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.5, 0.9285714285714286, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.2857142857142857, 0.6428571428571429, 0.0, ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.14285714285714285, 0.8571428571428571, 0.0,...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0, 0.7857142857142857, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999995</th>\n",
       "      <td>[0.5714285714285714, 0.6428571428571429, 0.0, ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999996</th>\n",
       "      <td>[0.5714285714285714, 0.5, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999997</th>\n",
       "      <td>[0.5714285714285714, 0.9285714285714286, 0.0, ...</td>\n",
       "      <td>[Prey, East]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999998</th>\n",
       "      <td>[0.7142857142857143, 0.07142857142857142, 0.0,...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999999</th>\n",
       "      <td>[0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       obs          lang\n",
       "0        [0.7857142857142857, 0.21428571428571427, 0.0,...            []\n",
       "1        [0.5, 0.9285714285714286, 0.0, 0.0, 0.0, 0.0, ...            []\n",
       "2        [0.2857142857142857, 0.6428571428571429, 0.0, ...            []\n",
       "3        [0.14285714285714285, 0.8571428571428571, 0.0,...            []\n",
       "4        [0.0, 0.7857142857142857, 0.0, 0.0, 0.0, 0.0, ...            []\n",
       "...                                                    ...           ...\n",
       "3999995  [0.5714285714285714, 0.6428571428571429, 0.0, ...            []\n",
       "3999996  [0.5714285714285714, 0.5, 0.0, 0.0, 0.0, 0.0, ...            []\n",
       "3999997  [0.5714285714285714, 0.9285714285714286, 0.0, ...  [Prey, East]\n",
       "3999998  [0.7142857142857143, 0.07142857142857142, 0.0,...            []\n",
       "3999999  [0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.0, ...            []\n",
       "\n",
       "[4000000 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df = pd.read_csv(\"../../results/data/lamarl_data/PPrgb_15.csv\", index_col=0)\n",
    "df = df.fillna('')\n",
    "df[\"lang\"] = df[\"lang\"].apply(lambda x: x.split(\" \"))\n",
    "df[\"obs\"] = df[\"obs\"].apply(json.loads)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "400f44bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7857142857142857,\n",
       " 0.21428571428571427,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"obs\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842d20ad",
   "metadata": {},
   "source": [
    "Make model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61d61cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "from src.algo.language.lm import GRUEncoder, GRUDecoder, OneHotEncoder\n",
    "from src.algo.nn_modules.mlp import MLPNetwork\n",
    "    \n",
    "\n",
    "class LanguageEncoder(nn.Module):\n",
    "\n",
    "    \"\"\" \n",
    "    Class to manage and train the language modules: the Language Encoder, the \n",
    "    Observation Encoder and the Decoder. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, context_dim, hidden_dim, embed_dim, policy_layer_N, \n",
    "                 lr, vocab, max_message_len, diff=False, device=\"cuda:0\"):\n",
    "        super(LanguageEncoder, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.word_encoder = OneHotEncoder(vocab, max_message_len)\n",
    "\n",
    "        self.lang_encoder = GRUEncoder(\n",
    "            context_dim, \n",
    "            hidden_dim, \n",
    "            embed_dim, \n",
    "            self.word_encoder,\n",
    "            device=device)\n",
    "\n",
    "        self.obs_encoder = MLPNetwork(\n",
    "            input_dim, context_dim, hidden_dim, policy_layer_N)\n",
    "\n",
    "        self.clip_loss = nn.CrossEntropyLoss()\n",
    "        # self.captioning_loss = nn.NLLLoss()\n",
    "\n",
    "        self.optim = torch.optim.Adam( \n",
    "            self.parameters(),\n",
    "            # list(self.lang_encoder.parameters()) +\n",
    "            # list(self.obs_encoder.parameters()) +\n",
    "            # list(self.decoder.parameters()),\n",
    "            lr=lr)\n",
    "\n",
    "    def prep_rollout(self, device):\n",
    "        self.device = device\n",
    "        self.eval()\n",
    "        self.to(self.device)\n",
    "        self.lang_encoder.device = self.device\n",
    "\n",
    "    def prep_training(self, device):\n",
    "        self.device = device\n",
    "        self.train()\n",
    "        self.to(self.device)\n",
    "        self.lang_encoder.device = self.device\n",
    "    \n",
    "    def store(self, obs, sent):\n",
    "        self.buffer.store(obs, sent)\n",
    "\n",
    "    def encode_sentences(self, sentence_batch):\n",
    "        \"\"\" \n",
    "        Encode a batch of sentences. \n",
    "        :param sentence_batch (list(list(int))): Batch of enoded sentences.\n",
    "\n",
    "        :return context_batch (torch.Tensor): Batch of context vectors, \n",
    "            dim=(batch_size, context_dim).\n",
    "        \"\"\"\n",
    "        context_batch = self.lang_encoder(sentence_batch).squeeze(0)\n",
    "        return context_batch\n",
    "\n",
    "    def get_save_dict(self):\n",
    "        save_dict = {\n",
    "            \"lang_encoder\": self.lang_encoder.state_dict(),\n",
    "            \"obs_encoder\": self.obs_encoder.state_dict()}\n",
    "        return save_dict\n",
    "\n",
    "    def load_params(self, save_dict):\n",
    "        self.lang_encoder.load_state_dict(save_dict[\"lang_encoder\"])\n",
    "        self.obs_encoder.load_state_dict(save_dict[\"obs_encoder\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0775417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim = 77\n",
    "context_dim = 2\n",
    "hidden_dim = 64\n",
    "embed_dim = 4\n",
    "n_layer = 2\n",
    "lr = 0.007\n",
    "vocab = [\"Prey\", \"Center\", \"North\", \"South\", \"East\", \"West\",\n",
    "                        \"Gem\", \"Yellow\", \"Green\", \"Purple\"]\n",
    "max_mess_len = 6\n",
    "\n",
    "ll = LanguageEncoder(obs_dim, context_dim, hidden_dim, embed_dim, n_layer, lr, vocab, max_mess_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f09687fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obs</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.7857142857142857, 0.21428571428571427, 0.0,...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.5, 0.9285714285714286, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.2857142857142857, 0.6428571428571429, 0.0, ...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.14285714285714285, 0.8571428571428571, 0.0,...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0, 0.7857142857142857, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999995</th>\n",
       "      <td>[0.5714285714285714, 0.6428571428571429, 0.0, ...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999996</th>\n",
       "      <td>[0.5714285714285714, 0.5, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999997</th>\n",
       "      <td>[0.5714285714285714, 0.9285714285714286, 0.0, ...</td>\n",
       "      <td>[2, 6, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999998</th>\n",
       "      <td>[0.7142857142857143, 0.07142857142857142, 0.0,...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999999</th>\n",
       "      <td>[0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       obs       lang\n",
       "0        [0.7857142857142857, 0.21428571428571427, 0.0,...        [1]\n",
       "1        [0.5, 0.9285714285714286, 0.0, 0.0, 0.0, 0.0, ...        [1]\n",
       "2        [0.2857142857142857, 0.6428571428571429, 0.0, ...        [1]\n",
       "3        [0.14285714285714285, 0.8571428571428571, 0.0,...        [1]\n",
       "4        [0.0, 0.7857142857142857, 0.0, 0.0, 0.0, 0.0, ...        [1]\n",
       "...                                                    ...        ...\n",
       "3999995  [0.5714285714285714, 0.6428571428571429, 0.0, ...        [1]\n",
       "3999996  [0.5714285714285714, 0.5, 0.0, 0.0, 0.0, 0.0, ...        [1]\n",
       "3999997  [0.5714285714285714, 0.9285714285714286, 0.0, ...  [2, 6, 1]\n",
       "3999998  [0.7142857142857143, 0.07142857142857142, 0.0,...        [1]\n",
       "3999999  [0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.0, ...        [1]\n",
       "\n",
       "[4000000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"lang\"] = ll.word_encoder.encode_batch(list(df[\"lang\"]))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c27ed8",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9143e997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000, -1.0000],\n",
      "        [ 1.0000, -1.0000],\n",
      "        [-1.0000,  1.0000],\n",
      "        ...,\n",
      "        [-0.9856,  0.9856],\n",
      "        [-1.0000,  1.0000],\n",
      "        [ 1.0000, -1.0000]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.7071, -0.7071],\n",
      "        [ 0.7071, -0.7071],\n",
      "        [-0.7071,  0.7071],\n",
      "        ...,\n",
      "        [-0.7071,  0.7071],\n",
      "        [-0.7071,  0.7071],\n",
      "        [ 0.7071, -0.7071]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([-0.0700, -0.0700,  0.0700,  0.0700,  0.0700,  0.0700,  0.0700,  0.0700,\n",
      "         0.0700,  0.0700,  0.0700,  0.0700, -0.0700, -0.0700,  0.0700, -0.0700,\n",
      "         0.0700,  0.0700, -0.0700,  0.0700,  0.0700, -0.0700,  0.0700, -0.0700,\n",
      "         0.0700,  0.0700,  0.0700,  0.0700, -0.0700,  0.0700,  0.0700, -0.0700,\n",
      "         0.0700, -0.0700,  0.0700,  0.0700,  0.0700, -0.0700,  0.0700, -0.0700,\n",
      "         0.0700,  0.0700,  0.0700,  0.0700,  0.0700,  0.0700, -0.0700,  0.0700,\n",
      "         0.0700,  0.0700,  0.0700,  0.0700,  0.0700, -0.0700,  0.0700,  0.0700,\n",
      "         0.0700,  0.0700,  0.0700, -0.0700, -0.0700,  0.0700, -0.0700,  0.0700,\n",
      "         0.0700,  0.0700,  0.0700,  0.0700,  0.0700, -0.0700,  0.0700,  0.0700,\n",
      "         0.0700, -0.0700,  0.0700,  0.0700,  0.0700,  0.0700,  0.0700, -0.0700,\n",
      "        -0.0700,  0.0700,  0.0700,  0.0700,  0.0700,  0.0700,  0.0700, -0.0700,\n",
      "        -0.0700,  0.0700,  0.0700,  0.0700, -0.0700,  0.0700, -0.0700,  0.0700,\n",
      "         0.0700,  0.0700,  0.0700,  0.0700, -0.0700,  0.0700, -0.0700,  0.0700,\n",
      "        -0.0700, -0.0700, -0.0700,  0.0700,  0.0700,  0.0700,  0.0700, -0.0700,\n",
      "         0.0700,  0.0700, -0.0700,  0.0700, -0.0700,  0.0700,  0.0700, -0.0700,\n",
      "        -0.0700,  0.0700,  0.0700,  0.0700, -0.0700, -0.0700,  0.0700,  0.0700,\n",
      "         0.0700, -0.0700,  0.0700,  0.0700,  0.0700,  0.0700,  0.0700, -0.0700,\n",
      "         0.0700,  0.0700,  0.0700, -0.0700,  0.0700, -0.0700, -0.0700, -0.0700,\n",
      "         0.0700,  0.0700,  0.0700,  0.0700, -0.0700,  0.0700,  0.0700, -0.0700,\n",
      "         0.0700, -0.0700, -0.0700, -0.0700,  0.0700,  0.0700,  0.0700, -0.0700,\n",
      "        -0.0700,  0.0700,  0.0700,  0.0700,  0.0700,  0.0700, -0.0700,  0.0700,\n",
      "         0.0700, -0.0700, -0.0700,  0.0700,  0.0700,  0.0700, -0.0700,  0.0700,\n",
      "        -0.0700,  0.0700,  0.0700, -0.0700,  0.0700,  0.0700,  0.0700,  0.0700,\n",
      "         0.0700,  0.0700,  0.0700,  0.0700,  0.0700, -0.0700,  0.0700,  0.0700,\n",
      "         0.0700, -0.0700, -0.0700,  0.0700,  0.0700, -0.0700,  0.0700,  0.0700,\n",
      "         0.0700,  0.0700,  0.0700, -0.0700,  0.0700,  0.0700,  0.0700, -0.0700,\n",
      "         0.0700,  0.0700,  0.0700, -0.0700,  0.0700,  0.0700,     nan, -0.0700,\n",
      "        -0.0700,  0.0700, -0.0700,  0.0700,  0.0700, -0.0700,  0.0700,  0.0700,\n",
      "         0.0700,  0.0700,  0.0700, -0.0700,  0.0700,  0.0700,  0.0700,  0.0700,\n",
      "         0.0700, -0.0700, -0.0700,  0.0700, -0.0700, -0.0700,  0.0700,  0.0700,\n",
      "         0.0700,  0.0700,  0.0700, -0.0700, -0.0700, -0.0700,  0.0700,  0.0700,\n",
      "        -0.0700,  0.0700,  0.0700, -0.0700, -0.0700, -0.0700,  0.0700, -0.0700,\n",
      "         0.0700, -0.0700,  0.0700, -0.0700, -0.0700, -0.0700, -0.0700, -0.0700,\n",
      "        -0.0700,  0.0700, -0.0700,  0.0700,  0.0700,  0.0700,  0.0700, -0.0700,\n",
      "         0.0700, -0.0700,  0.0700, -0.0700, -0.0700, -0.0700,  0.0700,  0.0700,\n",
      "         0.0700, -0.0700,  0.0700,  0.0700,  0.0700,  0.0700,  0.0700,  0.0700,\n",
      "         0.0700,  0.0700,  0.0700,  0.0700,  0.0700,  0.0700,  0.0700,  0.0700,\n",
      "         0.0700,  0.0700,  0.0700,  0.0700, -0.0700, -0.0700,  0.0700, -0.0700,\n",
      "         0.0700, -0.0700,  0.0700, -0.0700,  0.0700, -0.0700,  0.0700, -0.0700,\n",
      "         0.0700, -0.0700, -0.0700,  0.0700, -0.0700,  0.0700,  0.0700, -0.0700,\n",
      "         0.0700,  0.0700, -0.0700,  0.0700, -0.0700, -0.0700, -0.0700,  0.0700,\n",
      "        -0.0700,  0.0700, -0.0700,  0.0700, -0.0700, -0.0700,  0.0700,  0.0700,\n",
      "        -0.0700, -0.0700,  0.0700,  0.0700,  0.0700,  0.0700, -0.0700, -0.0700,\n",
      "        -0.0700,  0.0700,  0.0700,  0.0700, -0.0700, -0.0700, -0.0700,  0.0700,\n",
      "         0.0700,  0.0700,  0.0700, -0.0700,  0.0700,  0.0700,  0.0700, -0.0700,\n",
      "        -0.0700, -0.0700,  0.0700,  0.0700,  0.0700, -0.0700, -0.0700,  0.0700,\n",
      "         0.0700, -0.0700,  0.0700, -0.0700,  0.0700,  0.0700,  0.0700, -0.0700,\n",
      "         0.0700,  0.0700,  0.0700, -0.0700,  0.0700,  0.0700, -0.0700,  0.0700,\n",
      "         0.0700,  0.0700,  0.0700,  0.0700, -0.0700, -0.0700,  0.0700,  0.0700,\n",
      "         0.0700, -0.0700, -0.0700,  0.0700, -0.0700,  0.0700,  0.0700,  0.0700,\n",
      "         0.0700,  0.0700,  0.0700,  0.0700,  0.0700,  0.0700,  0.0700,  0.0700,\n",
      "        -0.0700,  0.0700,  0.0700,  0.0700, -0.0700,  0.0700,  0.0700,  0.0700,\n",
      "        -0.0700,  0.0700, -0.0700,  0.0700,  0.0700, -0.0700, -0.0700,  0.0700,\n",
      "        -0.0700,  0.0700,  0.0700,  0.0700,  0.0700, -0.0700,  0.0700,  0.0700,\n",
      "         0.0700,  0.0700,  0.0700,  0.0700, -0.0700,  0.0700,  0.0700,  0.0700,\n",
      "         0.0700, -0.0700, -0.0700,  0.0700, -0.0700,  0.0700, -0.0700,  0.0700,\n",
      "         0.0700,  0.0700, -0.0700,  0.0700, -0.0700,  0.0700,  0.0700,  0.0700,\n",
      "         0.0700,  0.0700,  0.0700,  0.0700,  0.0700,  0.0700, -0.0700,  0.0700,\n",
      "         0.0700,  0.0700,  0.0700,  0.0700,  0.0700, -0.0700,  0.0700,  0.0700,\n",
      "        -0.0700,  0.0700, -0.0700, -0.0700,  0.0700,  0.0700,  0.0700, -0.0700,\n",
      "        -0.0700,  0.0700,  0.0700, -0.0700, -0.0700,  0.0700, -0.0700,  0.0700,\n",
      "         0.0700,  0.0700,  0.0700,  0.0700,  0.0700,  0.0700,  0.0700,  0.0700,\n",
      "        -0.0700,  0.0700,  0.0700,  0.0700, -0.0700,  0.0700,  0.0700,  0.0700,\n",
      "         0.0700,  0.0700,  0.0700,  0.0700,  0.0700,  0.0700,  0.0700, -0.0700],\n",
      "       device='cuda:0', grad_fn=<DiagBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<DivBackward0>) nan\n"
     ]
    }
   ],
   "source": [
    "def clip(ll, enc_obs, enc_lang, temp, device):\n",
    "    # Compute similarity\n",
    "    norm_enc_obs = enc_obs / enc_obs.norm(\n",
    "        dim=1, keepdim=True)\n",
    "    norm_enc_lang = enc_lang / enc_lang.norm(\n",
    "        dim=1, keepdim=True)\n",
    "    sim = norm_enc_obs @ norm_enc_lang.t() * temp\n",
    "    mean_sim = sim.diag().mean()\n",
    "\n",
    "    print(enc_obs)\n",
    "    print(norm_enc_obs)\n",
    "    print(sim.diag())\n",
    "    print(mean_sim)\n",
    "\n",
    "    # Compute CLIP loss\n",
    "    labels = torch.arange(enc_obs.shape[0]).to(device)\n",
    "    loss_o = ll.clip_loss(sim, labels)\n",
    "    loss_l = ll.clip_loss(sim.t(), labels)\n",
    "    clip_loss = (loss_o + loss_l) / 2\n",
    "\n",
    "    return clip_loss, mean_sim.item()\n",
    "\n",
    "def train_lang_encoder(data, ll, n_epochs=50000, batch_size=512, temp=0.07, eval_every=1000,\n",
    "                       device=\"cuda:0\"):\n",
    "    train_data = data.iloc[:int(len(df) * 0.9)]\n",
    "    eval_data = data.iloc[int(len(df) * 0.9):]\n",
    "\n",
    "    clip_train_losses = []\n",
    "    clip_eval_losses = []\n",
    "    eval_sims = []\n",
    "\n",
    "    ll.prep_training(device)\n",
    "\n",
    "    for e_i in range(n_epochs):\n",
    "        sample = train_data.sample(n=batch_size)\n",
    "        \n",
    "        obs_batch = torch.Tensor(np.array(list(sample[\"obs\"]))).to(device)\n",
    "        lang_batch = list(sample[\"lang\"])\n",
    "    \n",
    "        enc_obs = ll.obs_encoder(obs_batch)\n",
    "        enc_lang = ll.encode_sentences(lang_batch)\n",
    "\n",
    "        clip_loss, mean_sim = clip(ll, enc_obs, enc_lang, temp, device)\n",
    "        print(clip_loss, mean_sim)\n",
    "        return\n",
    "    \n",
    "train_lang_encoder(df, ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08272535",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
