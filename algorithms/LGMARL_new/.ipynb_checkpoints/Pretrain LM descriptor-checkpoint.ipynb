{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16a95292",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c30253ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obs</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.7857142857142857, 0.21428571428571427, 0.0,...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.5, 0.9285714285714286, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.2857142857142857, 0.6428571428571429, 0.0, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.14285714285714285, 0.8571428571428571, 0.0,...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0, 0.7857142857142857, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399995</th>\n",
       "      <td>[0.14285714285714285, 0.7857142857142857, 0.0,...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399996</th>\n",
       "      <td>[0.6428571428571429, 0.6428571428571429, 0.0, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399997</th>\n",
       "      <td>[0.14285714285714285, 0.7857142857142857, 0.0,...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399998</th>\n",
       "      <td>[0.42857142857142855, 0.21428571428571427, 0.0...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399999</th>\n",
       "      <td>[0.14285714285714285, 0.8571428571428571, 0.0,...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      obs lang\n",
       "0       [0.7857142857142857, 0.21428571428571427, 0.0,...     \n",
       "1       [0.5, 0.9285714285714286, 0.0, 0.0, 0.0, 0.0, ...     \n",
       "2       [0.2857142857142857, 0.6428571428571429, 0.0, ...     \n",
       "3       [0.14285714285714285, 0.8571428571428571, 0.0,...     \n",
       "4       [0.0, 0.7857142857142857, 0.0, 0.0, 0.0, 0.0, ...     \n",
       "...                                                   ...  ...\n",
       "399995  [0.14285714285714285, 0.7857142857142857, 0.0,...     \n",
       "399996  [0.6428571428571429, 0.6428571428571429, 0.0, ...     \n",
       "399997  [0.14285714285714285, 0.7857142857142857, 0.0,...     \n",
       "399998  [0.42857142857142855, 0.21428571428571427, 0.0...     \n",
       "399999  [0.14285714285714285, 0.8571428571428571, 0.0,...     \n",
       "\n",
       "[400000 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../../results/data/lamarl_data/PPrgb_15.csv\", index_col=0)\n",
    "df = df.fillna('')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61d61cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "from src.algo.language.lm import GRUEncoder, GRUDecoder, OneHotEncoder\n",
    "from src.algo.nn_modules.mlp import MLPNetwork\n",
    "    \n",
    "\n",
    "class LanguageLearner(nn.Module):\n",
    "\n",
    "    \"\"\" \n",
    "    Class to manage and train the language modules: the Language Encoder, the \n",
    "    Observation Encoder and the Decoder. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args, vocab, max_message_len, diff=False, device=\"cpu\"):\n",
    "        super(LanguageLearner, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.word_encoder = OneHotEncoder(vocab, max_message_len)\n",
    "\n",
    "        self.lang_encoder = GRUEncoder(\n",
    "            args.context_dim, \n",
    "            args.lang_hidden_dim, \n",
    "            args.lang_embed_dim, \n",
    "            self.word_encoder,\n",
    "            device=device)\n",
    "\n",
    "        self.obs_encoder = MLPNetwork(\n",
    "            args.hidden_dim, args.context_dim, args.hidden_dim, args.policy_layer_N)\n",
    "\n",
    "        self.decoder = GRUDecoder(\n",
    "            args.context_dim, \n",
    "            args.lang_embed_dim, \n",
    "            self.word_encoder.enc_dim,\n",
    "            max_message_len, \n",
    "            embed_layer=self.lang_encoder.embed_layer,\n",
    "            use_gumbel=diff,\n",
    "            device=device)\n",
    "\n",
    "        self.clip_loss = nn.CrossEntropyLoss()\n",
    "        self.captioning_loss = nn.NLLLoss()\n",
    "\n",
    "        self.optim = torch.optim.Adam( \n",
    "            self.parameters(),\n",
    "            # list(self.lang_encoder.parameters()) +\n",
    "            # list(self.obs_encoder.parameters()) +\n",
    "            # list(self.decoder.parameters()),\n",
    "            lr=args.lang_lr)\n",
    "\n",
    "    def prep_rollout(self, device):\n",
    "        self.device = device\n",
    "        self.eval()\n",
    "        self.to(self.device)\n",
    "        self.lang_encoder.device = self.device\n",
    "        self.decoder.device = self.device\n",
    "\n",
    "    def prep_training(self, device):\n",
    "        self.device = device\n",
    "        self.train()\n",
    "        self.to(self.device)\n",
    "        self.lang_encoder.device = self.device\n",
    "        self.decoder.device = self.device\n",
    "    \n",
    "    def store(self, obs, sent):\n",
    "        self.buffer.store(obs, sent)\n",
    "\n",
    "    def encode_sentences(self, sentence_batch):\n",
    "        \"\"\" \n",
    "        Encode a batch of sentences. \n",
    "        :param sentence_batch (list(list(int))): Batch of enoded sentences.\n",
    "\n",
    "        :return context_batch (torch.Tensor): Batch of context vectors, \n",
    "            dim=(batch_size, context_dim).\n",
    "        \"\"\"\n",
    "        context_batch = self.lang_encoder(sentence_batch).squeeze(0)\n",
    "        return context_batch\n",
    "\n",
    "    def generate_sentences(self, context_batch, pad_max=False):\n",
    "        \"\"\" \n",
    "        Generate sentences from a batch of context vectors. \n",
    "        :param context_batch (np.ndarray): Batch of context vectors,\n",
    "            dim=(batch_size, context_dim).\n",
    "        \n",
    "        :return sentences (np.ndarray): Batch of generated sentences.\n",
    "        \"\"\"\n",
    "        # context_batch = torch.from_numpy(context_batch).to(self.device)\n",
    "        _, sentences = self.decoder(context_batch)\n",
    "\n",
    "        if pad_max and sentences.shape[1] < self.word_encoder.max_message_len:\n",
    "            sentences = np.concatenate(\n",
    "                (sentences, np.zeros(\n",
    "                    (sentences.shape[0], \n",
    "                     self.word_encoder.max_message_len - sentences.shape[1]),\n",
    "                    dtype=int)), \n",
    "                axis=-1)\n",
    "\n",
    "        return sentences\n",
    "\n",
    "    def get_save_dict(self):\n",
    "        save_dict = {\n",
    "            \"lang_encoder\": self.lang_encoder.state_dict(),\n",
    "            \"decoder\": self.decoder.state_dict()}\n",
    "        return save_dict\n",
    "\n",
    "    def load_params(self, save_dict):\n",
    "        self.lang_encoder.load_state_dict(save_dict[\"lang_encoder\"])\n",
    "        self.decoder.load_state_dict(save_dict[\"decoder\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0775417e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c27ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
