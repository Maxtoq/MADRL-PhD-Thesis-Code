{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7b774bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_pairs(data_path):\n",
    "    with open(data_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    pairs = []\n",
    "    for step, s_data in data.items():\n",
    "        if not step.startswith(\"Step\"):\n",
    "            continue\n",
    "        pairs.append({\n",
    "            \"observation\": s_data[\"Agent_0\"][\"Observation\"],\n",
    "            \"sentence\": s_data[\"Agent_0\"][\"Sentence\"][1:-1]\n",
    "        })\n",
    "        pairs.append({\n",
    "            \"observation\": s_data[\"Agent_1\"][\"Observation\"],\n",
    "            \"sentence\": s_data[\"Agent_1\"][\"Sentence\"][1:-1]\n",
    "        })\n",
    "    return pairs\n",
    "\n",
    "def init_training_objects(voc, context_dim, obs_dim, embed_dim, lr, do_embed=True):\n",
    "    word_encoder = OneHotEncoder(voc)\n",
    "\n",
    "    lang_enc = GRUEncoder(context_dim, 32, embed_dim, word_encoder, do_embed=do_embed)\n",
    "    dec = GRUDecoder(context_dim, embed_dim, word_encoder)\n",
    "\n",
    "    obs_enc = ObservationEncoder(obs_dim, context_dim)\n",
    "\n",
    "    cross_ent_l = nn.CrossEntropyLoss()\n",
    "    nll_l = nn.NLLLoss()\n",
    "    opt = optim.Adam(list(lang_enc.parameters()) + list(dec.parameters()) + list(obs_enc.parameters()), lr=lr)\n",
    "    \n",
    "    return word_encoder, lang_enc, obs_enc, dec, cross_ent_l, nll_l, opt\n",
    "\n",
    "def sample_batch(data, batch_size):\n",
    "    batch = random.sample(data, batch_size)\n",
    "    obs_batch = []\n",
    "    sent_batch = []\n",
    "    for pair in batch:\n",
    "        obs_batch.append(pair[\"observation\"])\n",
    "        sent_batch.append(pair[\"sentence\"])\n",
    "    return obs_batch, sent_batch\n",
    "\n",
    "def get_losses(obs_batch, sent_batch, obs_enc, lang_enc, dec, temp, cross_ent_loss, nll_loss, obs_learn_capt):\n",
    "    # Encode observations\n",
    "    obs_tensor = torch.Tensor(np.array(obs_batch))\n",
    "    context_batch = obs_enc(obs_tensor)\n",
    "\n",
    "    # Encode sentence\n",
    "    lang_context_batch = lang_enc(sent_batch)\n",
    "    lang_context_batch = lang_context_batch.squeeze()\n",
    "\n",
    "    # Compute similarity\n",
    "    norm_context_batch = context_batch / context_batch.norm(dim=1, keepdim=True)\n",
    "    lang_context_batch = lang_context_batch / lang_context_batch.norm(dim=1, keepdim=True)\n",
    "    sim = norm_context_batch @ lang_context_batch.t() * temp\n",
    "    mean_sim = sim.diag().mean()\n",
    "\n",
    "    # Compute loss\n",
    "    labels = torch.arange(len(obs_batch))\n",
    "    loss_o = cross_ent_loss(sim, labels)\n",
    "    loss_l = cross_ent_loss(sim.t(), labels)\n",
    "    clip_loss = (loss_o + loss_l) / 2\n",
    "    \n",
    "    # Decoding\n",
    "    encoded_targets = word_encoder.encode_batch(sent_batch)\n",
    "    if not obs_learn_capt:\n",
    "        context_batch = context_batch.detach()\n",
    "    decoder_outputs, _ = dec(context_batch, encoded_targets)\n",
    "\n",
    "    # Compute loss\n",
    "    dec_loss = 0\n",
    "    for d_o, e_t in zip(decoder_outputs, encoded_targets):\n",
    "        e_t = torch.argmax(e_t, dim=1)\n",
    "        dec_loss += nll_loss(d_o[:e_t.size(0)], e_t)\n",
    "    \n",
    "    return clip_loss, dec_loss, mean_sim\n",
    "\n",
    "def train(data, obs_enc, lang_enc, dec, word_encoder, cross_ent_loss, nll_loss, opt, \n",
    "          n_iters=80000, batch_size=128, temp=0.07, eval_data=None, eval_evry=1000,\n",
    "          sample_fn=sample_batch, clip_weight=1.0, capt_weight=1.0, obs_learn_capt=False):\n",
    "    start = time.time()\n",
    "    \n",
    "    clip_train_losses = []\n",
    "    clip_eval_losses = []\n",
    "    dec_train_losses = []\n",
    "    dec_eval_losses = []\n",
    "    eval_sims = []\n",
    "    \n",
    "    for s_i in tqdm(range(n_iters)):\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        # Sample batch\n",
    "        obs_batch, sent_batch = sample_fn(data, batch_size)\n",
    "        \n",
    "        # Compute both losses\n",
    "        clip_loss, dec_loss, _ = get_losses(obs_batch, sent_batch, obs_enc, lang_enc, dec, temp, \n",
    "                                         cross_ent_loss, nll_loss, obs_learn_capt)\n",
    "        \n",
    "        # Backprop\n",
    "        tot_loss = clip_weight * clip_loss + capt_weight * dec_loss\n",
    "        tot_loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        clip_train_losses.append(clip_loss.item())\n",
    "        dec_train_losses.append(dec_loss.item() / batch_size)\n",
    "        \n",
    "        if eval_data is not None and (s_i + 1) % eval_evry == 0:\n",
    "            with torch.no_grad():\n",
    "                # Sample batch\n",
    "                obs_batch, sent_batch = sample_fn(eval_data, batch_size)\n",
    "                \n",
    "                # Get both losses\n",
    "                clip_loss, dec_loss, sim = get_losses(\n",
    "                    obs_batch, sent_batch, obs_enc, lang_enc, dec, temp, cross_ent_loss, nll_loss, obs_learn_capt)\n",
    "                clip_eval_losses.append(clip_loss.item())\n",
    "                dec_eval_losses.append(dec_loss.item() / batch_size)\n",
    "                eval_sims.append(sim)\n",
    "    \n",
    "    return clip_train_losses, clip_eval_losses, dec_train_losses, dec_eval_losses, eval_sims\n",
    "\n",
    "def plot_curves(curves, titles):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        curves (list(list(list(float)))): list of list of training curves, each element i of the main list is a list\n",
    "            of all the training curves to plot in the subplot i.\n",
    "    \"\"\"\n",
    "    nb_subplots = len(curves)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, nb_subplots, figsize=(15,6))\n",
    "    if type(axs) is not np.ndarray:\n",
    "        axs = [axs]\n",
    "    for ax, plot, title in zip(axs, curves, titles):\n",
    "        max_len = max([len(c) for c in plot])\n",
    "        for c in plot:\n",
    "            c_len = len(c)\n",
    "            if c_len == max_len:\n",
    "                ax.plot(c)\n",
    "            else:\n",
    "                inter = max_len / c_len\n",
    "                ax.plot((np.arange(c_len) + 1) * inter, c)\n",
    "        ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ea1838a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "from src.algo.language.lm import GRUDecoder, init_rnn_params\n",
    "from src.algo.language.obs import ObservationEncoder\n",
    "\n",
    "\n",
    "class OneHotEncoder:\n",
    "    \"\"\"\n",
    "    Class managing the vocabulary and its one-hot encodings\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab, max_message_len=8):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            :param vocab (list): List of tokens that can appear in the language\n",
    "        \"\"\"\n",
    "        self.tokens = [\"<SOS>\", \"<EOS>\"] + vocab\n",
    "        self.enc_dim = len(self.tokens)\n",
    "        self.token_encodings = np.eye(self.enc_dim)\n",
    "        self.max_message_len = max_message_len + 1\n",
    "\n",
    "        self.SOS_ENC = self.token_encodings[0]\n",
    "        self.EOS_ENC = self.token_encodings[1]\n",
    "\n",
    "        self.SOS_ID = 0\n",
    "        self.EOS_ID = 1\n",
    "\n",
    "    def index2token(self, index):\n",
    "        \"\"\"\n",
    "        Returns the token corresponding to the given index in the vocabulary\n",
    "        Inputs:\n",
    "            :param index (int)\n",
    "        Outputs:\n",
    "            :param token (str)\n",
    "        \"\"\"\n",
    "        if type(index) in [list, np.ndarray]:\n",
    "            return [self.tokens[i] for i in index]\n",
    "        else:\n",
    "            return self.tokens[index]\n",
    "\n",
    "    def enc2token(self, encoding):\n",
    "        \"\"\"\n",
    "        Returns the token corresponding to the given one-hot encoding.\n",
    "        Inputs:\n",
    "            :param encoding (numpy.array): One-hot encoding.\n",
    "        Outputs:\n",
    "            :param token (str): Corresponding token.\n",
    "        \"\"\"\n",
    "        if len(encoding.shape) == 1:\n",
    "            return self.tokens[np.argmax(encoding)]\n",
    "        elif len(encoding.shape) == 2:\n",
    "            return [self.tokens[np.argmax(enconding[i])] for i in range(encoding.shape[0])]\n",
    "        else:\n",
    "            raise NotImplementedError(\"Wrong index type\")\n",
    "\n",
    "    def get_onehots(self, sentence):\n",
    "        \"\"\"\n",
    "        Transforms a sentence into a list of corresponding one-hot encodings\n",
    "        Inputs:\n",
    "            :param sentence (list): Input sentence, made of a list of tokens\n",
    "        Outputs:\n",
    "            :param onehots (list): List of one-hot encodings\n",
    "        \"\"\"\n",
    "        onehots = [\n",
    "            self.token_encodings[self.tokens.index(t)] \n",
    "            for t in sentence\n",
    "        ]\n",
    "        return onehots\n",
    "\n",
    "    def get_ids(self, sentence):\n",
    "        ids = [\n",
    "            self.tokens.index(t) \n",
    "            for t in sentence]\n",
    "        return ids\n",
    "    \n",
    "    def encode_batch(self, sentences, pad=False):\n",
    "        enc = []\n",
    "        for s in sentences:\n",
    "            enc_s = self.get_ids(s)\n",
    "            \n",
    "            enc_s.append(self.EOS_ID)\n",
    "            \n",
    "            if pad:\n",
    "                enc_s.extend([0] * (self.max_message_len - len(enc_s)))\n",
    "\n",
    "            enc.append(enc_s)\n",
    "        \n",
    "        if pad:        \n",
    "            enc = np.array(enc)\n",
    "        \n",
    "        return enc\n",
    "\n",
    "    def ids_to_onehots(self, ids_batch):\n",
    "        if type(ids_batch) is list:\n",
    "            onehots = [\n",
    "                self.token_encodings[ids]\n",
    "                for ids in ids_batch]\n",
    "            return onehots\n",
    "        elif type(ids_batch) is np.ndarray:\n",
    "            return self.token_encodings[ids]\n",
    "\n",
    "    def decode_batch(self, token_batch):\n",
    "        \"\"\"\n",
    "        Decode batch of encoded sentences\n",
    "        Inputs:\n",
    "            :param token_batch (list): List of encoded sentences.\n",
    "        Outputs:\n",
    "            :param decoded_batch (list): List of sentences.\n",
    "        \"\"\"\n",
    "        decoded_batch = []\n",
    "        for enc_sentence in token_batch:\n",
    "            sentence = []\n",
    "            for token in enc_sentence:\n",
    "                if type(token) is list:\n",
    "                    sentence.append(self.enc2token(token))\n",
    "                else:\n",
    "                    if token == 1:\n",
    "                        break\n",
    "                    sentence.append(self.index2token(token))\n",
    "            decoded_batch.append(sentence)\n",
    "        return decoded_batch\n",
    "\n",
    "class GRUEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Class for a language encoder using a Gated Recurrent Unit network\n",
    "    \"\"\"\n",
    "    def __init__(self, context_dim, hidden_dim, embed_dim, word_encoder, \n",
    "                 n_layers=1, device='cpu', do_embed=True):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            :param context_dim (int): Dimension of the context vectors (output\n",
    "                of the model).\n",
    "            :param hidden_dim (int): Dimension of the hidden state of the GRU\n",
    "                newtork.\n",
    "            :param word_encoder (OneHotEncoder): Word encoder, associating \n",
    "                tokens with one-hot encodings\n",
    "            :param n_layers (int): number of layers in the GRU (default: 1)\n",
    "            :param device (str): CUDA device\n",
    "        \"\"\"\n",
    "        super(GRUEncoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.word_encoder = word_encoder\n",
    "        self.context_dim = context_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.do_embed = do_embed\n",
    "        \n",
    "        self.embed_layer = nn.Embedding(self.word_encoder.enc_dim, embed_dim, padding_idx=0)\n",
    "        \n",
    "        if not self.do_embed:\n",
    "            embed_dim = self.word_encoder.enc_dim\n",
    "            \n",
    "        self.gru = nn.GRU(\n",
    "            embed_dim, \n",
    "            self.hidden_dim, \n",
    "            n_layers,\n",
    "            batch_first=True)\n",
    "        init_rnn_params(self.gru)\n",
    "        \n",
    "        self.out = nn.Linear(self.hidden_dim, context_dim)\n",
    "        self.norm = nn.LayerNorm(context_dim)\n",
    "        \n",
    "    def embed_sentences(self, sent_batch):\n",
    "        # Get one-hot encodings\n",
    "        enc_sent_batch = self.word_encoder.encode_batch(sent_batch)\n",
    "        \n",
    "        # Embed\n",
    "        if self.do_embed:\n",
    "            enc_ids_batch = [s.argmax(-1) for s in enc_sent_batch]\n",
    "            return [self.embed_layer(s) for s in enc_ids_batch]\n",
    "        else:\n",
    "            return enc_sent_batch\n",
    "\n",
    "    def forward(self, sent_batch):\n",
    "        \"\"\"\n",
    "        Transforms sentences into embeddings\n",
    "        Inputs:\n",
    "            :param sentence_batch (list(list(str))): Batch of sentences.\n",
    "        Outputs:\n",
    "            :param unsorted_hstates (torch.Tensor): Final hidden states\n",
    "                corresponding to each given sentence, dim=(1, batch_size, \n",
    "                context_dim)\n",
    "        \"\"\"\n",
    "        # Get one-hot encodings\n",
    "        enc_sent_batch = self.word_encoder.encode_batch(sent_batch)\n",
    "\n",
    "        # Get order of sententes sorted by length decreasing\n",
    "        ids = sorted(\n",
    "            range(len(enc_sent_batch)), \n",
    "            key=lambda x: len(enc_sent_batch[x]), \n",
    "            reverse=True)\n",
    "\n",
    "        # Sort the sentences by length\n",
    "        sorted_list = [enc_sent_batch[i] for i in ids]\n",
    "\n",
    "        # Embed\n",
    "        if self.do_embed:\n",
    "            # enc_ids_batch = [s.argmax(-1) for s in sorted_list]\n",
    "            model_input = [\n",
    "                self.embed_layer(torch.from_numpy(\n",
    "                    np.array(s)).to(self.device)) \n",
    "                for s in sorted_list]\n",
    "        else:\n",
    "            model_input = [\n",
    "                torch.Tensor(self.word_encoder.ids_to_onehots(s))\n",
    "                for s in sorted_list]\n",
    "\n",
    "        # Pad sentences\n",
    "        padded = nn.utils.rnn.pad_sequence(\n",
    "            model_input, batch_first=True)\n",
    "\n",
    "        # Pack padded sentences (to not care about padded tokens)\n",
    "        lens = [len(s) for s in sorted_list]\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            padded, lens, batch_first=True).to(self.device)\n",
    "\n",
    "        # Initial hidden state\n",
    "        hidden = torch.zeros(1, len(enc_sent_batch), self.hidden_dim, \n",
    "                        device=self.device)\n",
    "        \n",
    "        # Pass sentences into GRU model\n",
    "        _, hidden_states = self.gru(packed, hidden)\n",
    "\n",
    "        # Re-order hidden states\n",
    "        unsorted_hstates = torch.zeros_like(hidden_states).to(self.device)\n",
    "        unsorted_hstates[0,ids,:] = hidden_states[0,:,:]\n",
    "\n",
    "        return self.norm(self.out(unsorted_hstates))\n",
    "    \n",
    "    def forward_pad(self, sent_batch):\n",
    "        # Get one-hot encodings\n",
    "        enc_sent_batch = self.word_encoder.encode_batch(sent_batch, pad=True)\n",
    "        \n",
    "        # Embed\n",
    "        if self.do_embed:\n",
    "            # enc_ids_batch = [s.argmax(-1) for s in sorted_list]\n",
    "            model_input = []\n",
    "            for s in enc_sent_batch:\n",
    "                model_input.append(self.embed_layer(torch.from_numpy(s).to(self.device)))\n",
    "        else:\n",
    "            model_input = [\n",
    "                torch.Tensor(self.word_encoder.ids_to_onehots(s))\n",
    "                for s in enc_sent_batch]\n",
    "            \n",
    "        model_input = torch.stack(model_input, dim=0)\n",
    "            \n",
    "         # Initial hidden state\n",
    "        hidden = torch.zeros(1, len(enc_sent_batch), self.hidden_dim, \n",
    "                        device=self.device)\n",
    "        \n",
    "        # Pass sentences into GRU model\n",
    "        _, hidden_states = self.gru(model_input, hidden)\n",
    "        \n",
    "        return self.norm(self.out(hidden_states))\n",
    "        \n",
    "\n",
    "    def get_params(self):\n",
    "        return {'gru': self.gru.state_dict(),\n",
    "                'out': self.out.state_dict()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a0d14a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pairs = load_pairs(\"../MALNovelD/test_data/Sentences_Generated_P1.json\")\n",
    "\n",
    "train_data = data_pairs[:80000]\n",
    "test_data = data_pairs[80000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3311f9e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Located', 'Center'],\n",
       " ['Located', 'South', 'Landmark', 'South', 'West'],\n",
       " ['Located', 'East'],\n",
       " ['Located', 'South'],\n",
       " ['Located', 'South', 'West', 'Landmark', 'Not', 'Located', 'South', 'West'],\n",
       " ['Located', 'South'],\n",
       " ['Located', 'Center'],\n",
       " ['Located', 'North', 'East'],\n",
       " ['Located', 'South', 'West', 'Object', 'Not', 'Located', 'South', 'West'],\n",
       " ['Located', 'North']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = sample_batch(train_data, 10)\n",
    "s = b[1]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b8f1abbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 9, 1], [4, 2, 7, 2, 5, 1], [4, 10, 1], [4, 2, 1], [4, 2, 5, 7, 3, 4, 2, 5, 1], [4, 2, 1], [4, 9, 1], [4, 8, 10, 1], [4, 2, 5, 6, 3, 4, 2, 5, 1], [4, 8, 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 9.0619e-01, -8.8552e-01,  1.0731e+00, -1.3286e+00, -1.5643e-01,\n",
       "          -2.0196e+00,  1.3331e+00,  1.4312e+00, -3.4869e-01,  1.0863e+00,\n",
       "          -1.2101e+00,  5.3868e-01,  1.7657e-02, -1.0643e-01,  2.8892e-01,\n",
       "          -6.1975e-01],\n",
       "         [ 5.0001e-01, -4.8274e-01,  1.2174e+00, -9.5900e-01, -3.5529e-01,\n",
       "          -1.9555e+00,  1.2726e+00,  1.0162e+00, -1.1455e-01,  1.5178e+00,\n",
       "          -1.4768e+00,  8.6549e-01,  7.1660e-02,  2.7341e-01, -3.7624e-01,\n",
       "          -1.0144e+00],\n",
       "         [ 1.0943e+00, -1.0901e+00,  8.6732e-01, -1.5776e+00,  2.8836e-02,\n",
       "          -1.9289e+00,  8.7676e-01,  1.4712e+00, -1.6252e-03,  1.2815e+00,\n",
       "          -1.0892e+00,  4.4538e-01, -3.1260e-01, -6.4742e-02,  5.3654e-01,\n",
       "          -5.3716e-01],\n",
       "         [ 5.8758e-01, -1.0540e+00,  1.2938e+00, -1.4898e+00,  7.9003e-02,\n",
       "          -1.9677e+00,  1.2372e+00,  9.9669e-01,  1.8682e-01,  1.4630e+00,\n",
       "          -1.2511e+00,  3.0069e-01, -2.1805e-01, -2.7991e-03,  3.3289e-01,\n",
       "          -4.9434e-01],\n",
       "         [ 3.9548e-01, -6.6367e-01,  1.4449e+00, -9.1206e-01, -2.4862e-01,\n",
       "          -2.1564e+00,  1.4157e+00,  1.0884e+00, -1.7563e-01,  1.2352e+00,\n",
       "          -1.3647e+00,  6.4324e-01,  1.8453e-01,  1.2563e-01, -2.1171e-01,\n",
       "          -8.0028e-01],\n",
       "         [ 5.8758e-01, -1.0540e+00,  1.2938e+00, -1.4898e+00,  7.9003e-02,\n",
       "          -1.9677e+00,  1.2372e+00,  9.9669e-01,  1.8682e-01,  1.4630e+00,\n",
       "          -1.2511e+00,  3.0069e-01, -2.1805e-01, -2.7991e-03,  3.3289e-01,\n",
       "          -4.9434e-01],\n",
       "         [ 9.0619e-01, -8.8552e-01,  1.0731e+00, -1.3286e+00, -1.5643e-01,\n",
       "          -2.0196e+00,  1.3331e+00,  1.4312e+00, -3.4869e-01,  1.0863e+00,\n",
       "          -1.2101e+00,  5.3868e-01,  1.7657e-02, -1.0643e-01,  2.8892e-01,\n",
       "          -6.1975e-01],\n",
       "         [ 1.0159e+00, -1.2464e+00,  9.2048e-01, -1.6185e+00,  9.5356e-02,\n",
       "          -1.8853e+00,  9.7182e-01,  1.4318e+00,  5.6261e-02,  1.2331e+00,\n",
       "          -1.0102e+00,  1.7053e-01, -3.4827e-01, -1.8618e-01,  7.3264e-01,\n",
       "          -3.3307e-01],\n",
       "         [ 3.0463e-01, -6.5097e-01,  1.5265e+00, -8.2388e-01, -2.4475e-01,\n",
       "          -2.2199e+00,  1.4893e+00,  1.0773e+00, -2.4824e-01,  1.1021e+00,\n",
       "          -1.3451e+00,  6.0467e-01,  2.6994e-01,  1.1231e-01, -2.5098e-01,\n",
       "          -7.0277e-01],\n",
       "         [ 7.7060e-01, -1.3594e+00,  1.3338e+00, -1.5968e+00,  3.6591e-01,\n",
       "          -1.9553e+00,  1.1618e+00,  1.1757e+00,  9.9253e-02,  6.8597e-01,\n",
       "          -8.5085e-01, -7.7615e-02, -1.0705e-01, -4.9866e-01,  1.0158e+00,\n",
       "          -1.6309e-01]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_encoder, lang_enc, obs_enc, dec, cross_ent_l, nll_l, opt = init_training_objects(\n",
    "    ['South','Not','Located','West','Object','Landmark','North','Center','East'],\n",
    "    16, 17, 4, 0.007)\n",
    "print(word_encoder.encode_batch(s))\n",
    "lang_enc(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9a31abcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  9  1  0  0  0  0  0  0]\n",
      " [ 4  2  7  2  5  1  0  0  0]\n",
      " [ 4 10  1  0  0  0  0  0  0]\n",
      " [ 4  2  1  0  0  0  0  0  0]\n",
      " [ 4  2  5  7  3  4  2  5  1]\n",
      " [ 4  2  1  0  0  0  0  0  0]\n",
      " [ 4  9  1  0  0  0  0  0  0]\n",
      " [ 4  8 10  1  0  0  0  0  0]\n",
      " [ 4  2  5  6  3  4  2  5  1]\n",
      " [ 4  8  1  0  0  0  0  0  0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5276, -0.7580,  1.1326, -1.2639, -0.2393, -1.5842,  1.6884,\n",
       "           0.7895, -0.4071,  1.6681, -1.5560,  0.6566, -0.2015,  0.1352,\n",
       "           0.0038, -0.5918],\n",
       "         [ 0.5373, -0.7058,  1.1374, -1.1945, -0.2169, -1.6560,  1.6617,\n",
       "           0.8352, -0.4447,  1.5779, -1.5986,  0.7312, -0.1344,  0.1914,\n",
       "          -0.1002, -0.6211],\n",
       "         [ 0.5452, -0.7639,  1.1230, -1.2780, -0.2353, -1.5753,  1.6722,\n",
       "           0.7911, -0.3969,  1.6827, -1.5535,  0.6526, -0.2114,  0.1346,\n",
       "           0.0120, -0.5989],\n",
       "         [ 0.5339, -0.7655,  1.1073, -1.2632, -0.2282, -1.5757,  1.6820,\n",
       "           0.7859, -0.3912,  1.6985, -1.5643,  0.6400, -0.2186,  0.1418,\n",
       "           0.0062, -0.5888],\n",
       "         [ 0.3955, -0.6637,  1.4449, -0.9121, -0.2486, -2.1564,  1.4157,\n",
       "           1.0884, -0.1756,  1.2352, -1.3647,  0.6432,  0.1845,  0.1256,\n",
       "          -0.2117, -0.8003],\n",
       "         [ 0.5339, -0.7655,  1.1073, -1.2632, -0.2282, -1.5757,  1.6820,\n",
       "           0.7859, -0.3912,  1.6985, -1.5643,  0.6400, -0.2186,  0.1418,\n",
       "           0.0062, -0.5888],\n",
       "         [ 0.5276, -0.7580,  1.1326, -1.2639, -0.2393, -1.5842,  1.6884,\n",
       "           0.7895, -0.4071,  1.6681, -1.5560,  0.6566, -0.2015,  0.1352,\n",
       "           0.0038, -0.5918],\n",
       "         [ 0.5355, -0.7877,  1.1311, -1.2671, -0.2351, -1.5904,  1.6863,\n",
       "           0.7943, -0.3993,  1.6733, -1.5478,  0.6298, -0.2043,  0.1319,\n",
       "           0.0165, -0.5670],\n",
       "         [ 0.3046, -0.6510,  1.5265, -0.8239, -0.2447, -2.2199,  1.4893,\n",
       "           1.0773, -0.2482,  1.1021, -1.3451,  0.6047,  0.2699,  0.1123,\n",
       "          -0.2510, -0.7028],\n",
       "         [ 0.4985, -0.7900,  1.1284, -1.2416, -0.2346, -1.5888,  1.7190,\n",
       "           0.7811, -0.3983,  1.6837, -1.5574,  0.6090, -0.2065,  0.1357,\n",
       "           0.0101, -0.5483]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(word_encoder.encode_batch(s, pad=True))\n",
    "lang_enc.forward_pad(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4b3c96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
